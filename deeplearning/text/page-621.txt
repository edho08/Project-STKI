CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

18.1 The Log-Likelihood Gradient

What makes learning undirected models by maximum likelihood particularly difficult is that the partition function depends on the parameters. The gradient of the log-likelihood with respect to the parameters has a term corresponding to the gradient of the partition function:

 log p(x; ) =  log p~(x; ) -  log Z().

(18.4)

This is a well-known decomposition into the positive phase and negative phase of learning.
For most undirected models of interest, the negative phase is difficult. Models with no latent variables or with few interactions between latent variables typically have a tractable positive phase. The quintessential example of a model with a straightforward positive phase and difficult negative phase is the RBM, which has hidden units that are conditionally independent from each other given the visible units. The case where the positive phase is difficult, with complicated interactions between latent variables, is primarily covered in chapter 19. This chapter focuses on the difficulties of the negative phase.
Let us look more closely at the gradient of log Z:

 log Z

(18.5)

= Z

Z

=  x p~(x)

Z

=

x

p~(x) Z

.

(18.6) (18.7) (18.8)

For models that guarantee p(x) > 0 for all x, we can substitute exp (log p~(x))

for p~(x):


x



exp

(log

p~(x))

=


x

exp

(log

Z p~(x))



log p~(x)

=


x

Z p~(x) 

log

p~(x)

Z



= p(x) log p~(x)

(18.9) (18.10) (18.11) (18.12)

x

606

