CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS
with the more advanced models in part III. For example, contrastive divergence gives a technique for approximating the gradient of the intractable log-likelihood of a Boltzmann machine.
Various neural network optimization algorithms are designed to account for imperfections in the gradient estimate. One can also avoid the problem by choosing a surrogate loss function that is easier to approximate than the true loss.
8.2.7 Poor Correspondence between Local and Global Structure
Many of the problems we have discussed so far correspond to properties of the loss function at a single point--it can be difficult to make a single step if J ( ) is poorly conditioned at the current point , or if  lies on a cliff, or if  is a saddle point hiding the opportunity to make progress downhill from the gradient.
It is possible to overcome all of these problems at a single point and still perform poorly if the direction that results in the most improvement locally does not point toward distant regions of much lower cost.
Goodfellow et al. (2015) argue that much of the runtime of training is due to the length of the trajectory needed to arrive at the solution. Figure 8.2 shows that the learning trajectory spends most of its time tracing out a wide arc around a mountain-shaped structure.
Much of research into the difficulties of optimization has focused on whether training arrives at a global minimum, a local minimum, or a saddle point, but in practice neural networks do not arrive at a critical point of any kind. Figure 8.1 shows that neural networks often do not arrive at a region of small gradient. Indeed, such critical points do not even necessarily exist. For example, the loss function - log p( y | x; ) can lack a global minimum point and instead asymptotically approach some value as the model becomes more confident. For a classifier with discrete y and p(y | x) provided by a softmax, the negative log-likelihood can become arbitrarily close to zero if the model is able to correctly classify every example in the training set, but it is impossible to actually reach the value of zero. Likewise, a model of real values p(y | x) = N (y; f (),  -1) can have negative log-likelihood that asymptotes to negative infinity--if f() is able to correctly predict the value of all training set y targets, the learning algorithm will increase  without bound. See figure 8.4 for an example of a failure of local optimization to find a good cost function value even in the absence of any local minima or saddle points.
Future research will need to develop further understanding of the factors that influence the length of the learning trajectory and better characterize the outcome
291

