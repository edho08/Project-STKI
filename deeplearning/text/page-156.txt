CHAPTER 5. MACHINE LEARNING BASICS

This same strategy can be applied to essentially any supervised learning problem, by writing down a parametric family of conditional probability distributions over the right kind of input and output variables.

5.7.2 Support Vector Machines

One of the most influential approaches to supervised learning is the support vector
machine (Boser et al., 1992; Cortes and Vapnik, 1995). This model is similar to logistic regression in that it is driven by a linear function wx + b. Unlike logistic
regression, the support vector machine does not provide probabilities, but only
outputs a class identity. The SVM predicts that the positive class is present when w x + b is positive. Likewise, it predicts that the negative class is present when w x + b is negative.

One key innovation associated with support vector machines is the kernel

trick. The kernel trick consists of observing that many machine learning algorithms

can be written exclusively in terms of dot products between examples. For example,

it can be shown that the linear function used by the support vector machine can

be re-written as

wx + b = b +  m ixx(i)

(5.82)

i=1

where x(i) is a training example and  is a vector of coefficients. Rewriting the

learning algorithm this way allows us to replace x by the output of a given feature function (x) and the dot product with a function k(x, x(i)) = (x)·  (x(i) ) called a kernel. The · operator represents an inner product analogous to (x)(x(i)).

For some feature spaces, we may not use literally the vector inner product. In

some infinite dimensional spaces, we need to use other kinds of inner products, for

example, inner products based on integration rather than summation. A complete

development of these kinds of inner products is beyond the scope of this book.

After replacing dot products with kernel evaluations, we can make predictions

using the function

f (x)

=

b

+

 ik(x,

x(i)

).

(5.83)

i

This function is nonlinear with respect to x, but the relationship between (x)

and f (x) is linear. Also, the relationship between  and f(x) is linear. The

kernel-based function is exactly equivalent to preprocessing the data by applying

(x) to all inputs, then learning a linear model in the new transformed space.

The kernel trick is powerful for two reasons. First, it allows us to learn models that are nonlinear as a function of x using convex optimization techniques that are

141

