CHAPTER 20. DEEP GENERATIVE MODELS

Figure 20.6: Examples of two-dimensional coordinate systems for high-dimensional manifolds, learned by a variational autoencoder (Kingma and Welling, 2014a). Two dimensions may be plotted directly on the page for visualization, so we can gain an understanding of how the model works by training a model with a 2-D latent code, even if we believe the intrinsic dimensionality of the data manifold is much higher. The images shown are not examples from the training set but images x actually generated by the model p(x | z), simply by changing the 2-D "code" z (each image corresponds to a different choice of "code" z on a 2-D uniform grid). (Left)The two-dimensional map of the Frey faces manifold. One dimension that has been discovered (horizontal) mostly corresponds to a rotation of the face, while the other (vertical) corresponds to the emotional expression. (Right)The two-dimensional map of the MNIST manifold.

This drives the discriminator to attempt to learn to correctly classify samples as real

or fake. Simultaneously, the generator attempts to fool the classifier into believing

its samples are real. At convergence, the generator's samples are indistinguishable

from

real

data,

and

the

discriminator

outputs

1 2

everywhere.

The

discriminator

may then be discarded.

The main motivation for the design of GANs is that the learning process
requires neither approximate inference nor approximation of a partition function gradient. In the case where maxd v(g, d) is convex in (g) (such as the case where optimization is performed directly in the space of probability density functions)
the procedure is guaranteed to converge and is asymptotically consistent.

Unfortunately, learning in GANs can be difficult in practice when g and d are represented by neural networks and maxd v(g, d) is not convex. Goodfellow

700

