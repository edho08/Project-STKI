CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS
example, it is common to make the Markov assumption that the graphical model should only contain edges from {y(t-k), . . . , y(t-1)} to y(t), rather than containing edges from the entire past history. However, in some cases, we believe that all past inputs should have an influence on the next element of the sequence. RNNs are useful when we believe that the distribution over y(t) may depend on a value of y(i) from the distant past in a way that is not captured by the effect of y(i) on y(t-1).
One way to interpret an RNN as a graphical model is to view the RNN as defining a graphical model whose structure is the complete graph, able to represent direct dependencies between any pair of y values. The graphical model over the y values with the complete graph structure is shown in figure 10.7. The complete graph interpretation of the RNN is based on ignoring the hidden units h(t) by marginalizing them out of the model.
It is more interesting to consider the graphical model structure of RNNs that results from regarding the hidden units h(t) as random variables.1 Including the hidden units in the graphical model reveals that the RNN provides a very efficient parametrization of the joint distribution over the observations. Suppose that we represented an arbitrary joint distribution over discrete values with a tabular representation--an array containing a separate entry for each possible assignment of values, with the value of that entry giving the probability of that assignment occurring. If y can take on k different values, the tabular representation would have O(k ) parameters. By comparison, due to parameter sharing, the number of parameters in the RNN is O(1) as a function of sequence length. The number of parameters in the RNN may be adjusted to control model capacity but is not forced to scale with sequence length. Equation 10.5 shows that the RNN parametrizes long-term relationships between variables efficiently, using recurrent applications of the same function f and same parameters  at each time step. Figure 10.8 illustrates the graphical model interpretation. Incorporating the h(t) nodes in the graphical model decouples the past and the future, acting as an intermediate quantity between them. A variable y(i) in the distant past may influence a variable y(t) via its effect on h. The structure of this graph shows that the model can be efficiently parametrized by using the same conditional probability distributions at each time step, and that when the variables are all observed, the probability of the joint assignment of all variables can be evaluated efficiently.
Even with the efficient parametrization of the graphical model, some operations remain computationally challenging. For example, it is difficult to predict missing
1
The conditional distribution over these variables given their parents is deterministic. This is perfectly legitimate, though it is somewhat rare to design a graphical model with such deterministic
hidden units.
389

