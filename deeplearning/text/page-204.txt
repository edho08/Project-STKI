CHAPTER 6. DEEP FEEDFORWARD NETWORKS

Regardless of whether we use standard deviation, variance, or precision, we must ensure that the covariance matrix of the Gaussian is positive definite. Because the eigenvalues of the precision matrix are the reciprocals of the eigenvalues of the covariance matrix, this is equivalent to ensuring that the precision matrix is positive definite. If we use a diagonal matrix, or a scalar times the diagonal matrix, then the only condition we need to enforce on the output of the model is positivity. If we suppose that a is the raw activation of the model used to determine the diagonal precision, we can use the softplus function to obtain a positive precision vector:  = (a). This same strategy applies equally if using variance or standard deviation rather than precision or if using a scalar times identity rather than diagonal matrix.

It is rare to learn a covariance or precision matrix with richer structure than
diagonal. If the covariance is full and conditional, then a parametrization must
be chosen that guarantees positive-definiteness of the predicted covariance matrix. This can be achieved by writing (x) = B(x)B (x), where B is an unconstrained
square matrix. One practical issue if the matrix is full rank is that computing the likelihood is expensive, with a d × d matrix requiring O(d3 ) computation for the determinant and inverse of (x) (or equivalently, and more commonly done, its
eigendecomposition or that of B(x)).

We often want to perform multimodal regression, that is, to predict real values that come from a conditional distribution p(y | x) that can have several different peaks in y space for the same value of x. In this case, a Gaussian mixture is a natural representation for the output (Jacobs et al., 1991; Bishop, 1994). Neural networks with Gaussian mixtures as their output are often called mixture density networks. A Gaussian mixture output with n components is defined by the conditional probability distribution

p(y | x) = n p(c = i | x)N (y; µ(i)(x), (i) (x)).
i=1

(6.35)

The neural network must have three outputs: a vector defining p(c = i | x), a matrix providing µ(i)(x) for all i, and a tensor providing (i)( x) for all i. These
outputs must satisfy different constraints:

1. Mixture components p(c = i | x): these form a multinoulli distribution over the n different components associated with latent variable1 c, and can

1
We

consider

c

to

be

latent

because

we

do

not

observe

it

in

the

data:

given

input

x

and

target

y , it is not possible to know with certainty which Gaussian component was responsible for y, but

we can imagine that y was generated by picking one of them, and make that unobserved choice a

random variable.

189

