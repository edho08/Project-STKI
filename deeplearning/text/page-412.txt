CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS
answering, where the input and output sequences in the training set are generally not of the same length (although their lengths might be related).
We often call the input to the RNN the "context." We want to produce a representation of this context, C . The context C might be a vector or sequence of vectors that summarize the input sequence X = (x(1), . . . , x(nx)).
The simplest RNN architecture for mapping a variable-length sequence to another variable-length sequence was first proposed by Cho et al. (2014a) and shortly after by Sutskever et al. (2014), who independently developed that architecture and were the first to obtain state-of-the-art translation using this approach. The former system is based on scoring proposals generated by another machine translation system, while the latter uses a standalone recurrent network to generate the translations. These authors respectively called this architecture, illustrated in figure 10.12, the encoder-decoder or sequence-to-sequence architecture. The idea is very simple: (1) an encoder or reader or input RNN processes the input sequence. The encoder emits the context C , usually as a simple function of its final hidden state. (2) a decoder or writer or output RNN is conditioned on that fixed-length vector (just like in figure 10.9) to generate the output sequence Y = (y(1), . . . , y(ny )). The innovation of this kind of architecture over those presented in earlier sections of this chapter is that the lengths nx and ny can vary from each other, while previous architectures constrained nx = ny = . In a sequence-to-sequence architecture, the two RNNs are trained jointly to maximize the average of log P (y(1), . . . , y(ny ) | x(1), . . . , x(nx)) over all the pairs of x and y sequences in the training set. The last state hnx of the encoder RNN is typically used as a representation C of the input sequence that is provided as input to the decoder RNN.
If the context C is a vector, then the decoder RNN is simply a vector-tosequence RNN as described in section 10.2.4. As we have seen, there are at least two ways for a vector-to-sequence RNN to receive input. The input can be provided as the initial state of the RNN, or the input can be connected to the hidden units at each time step. These two ways can also be combined.
There is no constraint that the encoder must have the same size of hidden layer as the decoder.
One clear limitation of this architecture is when the context C output by the encoder RNN has a dimension that is too small to properly summarize a long sequence. This phenomenon was observed by Bahdanau et al. (2015) in the context of machine translation. They proposed to make C a variable-length sequence rather than a fixed-size vector. Additionally, they introduced an attention mechanism that learns to associate elements of the sequence C to elements of the output
397

