CHAPTER 19. APPROXIMATE INFERENCE

MAP inference is commonly used in deep learning as both a feature extractor and a learning mechanism. It is primarily used for sparse coding models.

Recall from section 13.4 that sparse coding is a linear factor model that imposes

a sparsity-inducing prior on its hidden units. A common choice is a factorial Laplace

prior, with

p(hi) =

 e-|hi|. 2

(19.14)

The visible units are then generated by performing a linear transformation and

adding noise:

p(x | h) = N (v; W h + b, -1I).

(19.15)

Computing or even representing p(h | v) is difficult. Every pair of variables hi and hj are both parents of v. This means that when v is observed, the graphical model contains an active path connecting hi and hj. All of the hidden units thus participate in one massive clique in p(h | v). If the model were Gaussian then these interactions could be modeled efficiently via the covariance matrix, but the
sparse prior makes these interactions non-Gaussian.

Because p(h | v) is intractable, so is the computation of the log-likelihood and its gradient. We thus cannot use exact maximum likelihood learning. Instead, we use MAP inference and learn the parameters by maximizing the ELBO defined by the Dirac distribution around the MAP estimate of h.

If we concatenate all of the h vectors in the training set into a matrix H, and concatenate all of the v vectors into a matrix V , then the sparse coding learning process consists of minimizing

J (H, W )

=





|Hi,j | +

V

- H W  2

.

i,j

i,j

i,j

(19.16)

Most applications of sparse coding also involve weight decay or a constraint on the norms of the columns of W, in order to prevent the pathological solution with extremely small H and large W .
We can minimize J by alternating between minimization with respect to H and minimization with respect to W . Both sub-problems are convex. In fact, the minimization with respect to W is just a linear regression problem. However, minimization of J with respect to both arguments is usually not a convex problem.
Minimization with respect to H requires specialized algorithms such as the feature-sign search algorithm (Lee et al., 2007).

637

