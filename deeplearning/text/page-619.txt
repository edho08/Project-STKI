CHAPTER 17. MONTE CARLO METHODS
17.5.2 Depth May Help Mixing
When drawing samples from a latent variable model p(h, x), we have seen that if p(h | x) encodes x too well, then sampling from p(x | h) will not change x very much and mixing will be poor. One way to resolve this problem is to make h be a deep representation, that encodes x into h in such a way that a Markov chain in the space of h can mix more easily. Many representation learning algorithms, such as autoencoders and RBMs, tend to yield a marginal distribution over h that is more uniform and more unimodal than the original data distribution over x. It can be argued that this arises from trying to minimize reconstruction error while using all of the available representation space, because minimizing reconstruction error over the training examples will be better achieved when different training examples are easily distinguishable from each other in h-space, and thus well separated. Bengio et al. (2013a) observed that deeper stacks of regularized autoencoders or RBMs yield marginal distributions in the top-level h-space that appeared more spread out and more uniform, with less of a gap between the regions corresponding to different modes (categories, in the experiments). Training an RBM in that higher-level space allowed Gibbs sampling to mix faster between modes. It remains however unclear how to exploit this observation to help better train and sample from deep generative models.
Despite the difficulty of mixing, Monte Carlo techniques are useful and are often the best tool available. Indeed, they are the primary tool used to confront the intractable partition function of undirected models, discussed next.
604

