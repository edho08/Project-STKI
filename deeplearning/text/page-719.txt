CHAPTER 20. DEEP GENERATIVE MODELS
dynamically updated discriminator that automatically focuses its attention on whichever statistic the generator network is matching the least effectively.
Instead, generative moment matching networks can be trained by minimizing a cost function called maximum mean discrepancy (Schölkopf and Smola, 2002; Gretton et al., 2012) or MMD. This cost function measures the error in the first moments in an infinite-dimensional space, using an implicit mapping to feature space defined by a kernel function in order to make computations on infinite-dimensional vectors tractable. The MMD cost is zero if and only if the two distributions being compared are equal.
Visually, the samples from generative moment matching networks are somewhat disappointing. Fortunately, they can be improved by combining the generator network with an autoencoder. First, an autoencoder is trained to reconstruct the training set. Next, the encoder of the autoencoder is used to transform the entire training set into code space. The generator network is then trained to generate code samples, which may be mapped to visually pleasing samples via the decoder.
Unlike GANs, the cost function is defined only with respect to a batch of examples from both the training set and the generator network. It is not possible to make a training update as a function of only one training example or only one sample from the generator network. This is because the moments must be computed as an empirical average across many samples. When the batch size is too small, MMD can underestimate the true amount of variation in the distributions being sampled. No finite batch size is sufficiently large to eliminate this problem entirely, but larger batches reduce the amount of underestimation. When the batch size is too large, the training procedure becomes infeasibly slow, because many examples must be processed in order to compute a single small gradient step.
As with GANs, it is possible to train a generator net using MMD even if that generator net assigns zero probability to the training points.
20.10.6 Convolutional Generative Networks
When generating images, it is often useful to use a generator network that includes a convolutional structure (see for example Goodfellow et al. (2014c) or Dosovitskiy et al. (2015)). To do so, we use the "transpose" of the convolution operator, described in section 9.5. This approach often yields more realistic images and does so using fewer parameters than using fully connected layers without parameter sharing.
Convolutional networks for recognition tasks have information flow from the image to some summarization layer at the top of the network, often a class label.
704

