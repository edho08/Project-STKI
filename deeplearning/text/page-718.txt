CHAPTER 20. DEEP GENERATIVE MODELS

Gaussian distribution.
Dropout seems to be important in the discriminator network. In particular, units should be stochastically dropped while computing the gradient for the generator network to follow. Following the gradient of the deterministic version of the discriminator with its weights divided by two does not seem to be as effective. Likewise, never using dropout seems to yield poor results.
While the GAN framework is designed for differentiable generator networks, similar principles can be used to train other kinds of models. For example, selfsupervised boosting can be used to train an RBM generator to fool a logistic regression discriminator (Welling et al., 2002).

20.10.5 Generative Moment Matching Networks

Generative moment matching networks (Li et al., 2015; Dziugaite et al., 2015) are another form of generative model based on differentiable generator networks. Unlike VAEs and GANs, they do not need to pair the generator network with any other network--neither an inference network as used with VAEs nor a discriminator network as used with GANs.
These networks are trained with a technique called moment matching. The basic idea behind moment matching is to train the generator in such a way that many of the statistics of samples generated by the model are as similar as possible to those of the statistics of the examples in the training set. In this context, a moment is an expectation of different powers of a random variable. For example, the first moment is the mean, the second moment is the mean of the squared values, and so on. In multiple dimensions, each element of the random vector may be raised to different powers, so that a moment may be any quantity of the form

E
x

i

xni i

(20.82)

where n = [n1, n2, . . . , nd] is a vector of non-negative integers.
Upon first examination, this approach seems to be computationally infeasible. For example, if we want to match all the moments of the form xixj, then we need to minimize the difference between a number of values that is quadratic in the dimension of x. Moreover, even matching all of the first and second moments would only be sufficient to fit a multivariate Gaussian distribution, which captures only linear relationships between values. Our ambitions for neural networks are to capture complex nonlinear relationships, which would require far more moments. GANs avoid this problem of exhaustively enumerating all moments by using a

703

