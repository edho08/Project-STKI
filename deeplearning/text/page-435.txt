CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS
mechanism for choosing an address is in its form identical to the attention mechanism which had been previously introduced in the context of machine translation (Bahdanau et al., 2015) and discussed in section 12.4.5.1. The idea of attention mechanisms for neural networks was introduced even earlier, in the context of handwriting generation (Graves, 2013), with an attention mechanism that was constrained to move only forward in time through the sequence. In the case of machine translation and memory networks, at each step, the focus of attention can move to a completely different place, compared to the previous step.
Recurrent neural networks provide a way to extend deep learning to sequential data. They are the last major tool in our deep learning toolbox. Our discussion now moves to how to choose and use these tools and how to apply them to real-world tasks.
420

