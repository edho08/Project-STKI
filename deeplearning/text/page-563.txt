CHAPTER 15. REPRESENTATION LEARNING
values describing each input, but they can not be controlled separately from each other, so this does not qualify as a true distributed representation.
 Decision trees: only one leaf (and the nodes on the path from root to leaf) is activated when an input is given.
 Gaussian mixtures and mixtures of experts: the templates (cluster centers) or experts are now associated with a degree of activation. As with the k-nearest neighbors algorithm, each input is represented with multiple values, but those values cannot readily be controlled separately from each other.
 Kernel machines with a Gaussian kernel (or other similarly local kernel): although the degree of activation of each "support vector" or template example is now continuous-valued, the same issue arises as with Gaussian mixtures.
 Language or translation models based on n-grams. The set of contexts (sequences of symbols) is partitioned according to a tree structure of suffixes. A leaf may correspond to the last two words being w1 and w2, for example. Separate parameters are estimated for each leaf of the tree (with some sharing being possible).
For some of these non-distributed algorithms, the output is not constant by parts but instead interpolates between neighboring regions. The relationship between the number of parameters (or examples) and the number of regions they can define remains linear.
An important related concept that distinguishes a distributed representation from a symbolic one is that generalization arises due to shared attributes between different concepts. As pure symbols, " cat" and " dog" are as far from each other as any other two symbols. However, if one associates them with a meaningful distributed representation, then many of the things that can be said about cats can generalize to dogs and vice-versa. For example, our distributed representation may contain entries such as "has_fur" or "number_of_legs" that have the same value for the embedding of both " cat" and "dog." Neural language models that operate on distributed representations of words generalize much better than other models that operate directly on one-hot representations of words, as discussed in section 12.4. Distributed representations induce a rich similarity space, in which semantically close concepts (or inputs) are close in distance, a property that is absent from purely symbolic representations.
When and why can there be a statistical advantage from using a distributed representation as part of a learning algorithm? Distributed representations can
548

