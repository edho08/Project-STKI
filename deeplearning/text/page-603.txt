CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

h1

h2

h3

h4

v1

v2

v3

Figure 16.14: An RBM drawn as a Markov network.

and

p(v | h) = ip(vi | h).

(16.12)

The individual conditionals are simple to compute as well. For the binary RBM

we obtain:





P (hi = 1 | v) =  vW:,i + bi ,





P (hi = 0 | v) = 1 -  v W:,i + bi .

(16.13) (16.14)

Together these properties allow for efficient block Gibbs sampling, which alternates between sampling all of h simultaneously and sampling all of v simultaneously. Samples generated by Gibbs sampling from an RBM model are shown in figure 16.15.
Since the energy function itself is just a linear function of the parameters, it is easy to take its derivatives. For example,

 Wi,j E(v, h) = -vihj.

(16.15)

These two properties--efficient Gibbs sampling and efficient derivatives--make training convenient. In chapter 18, we will see that undirected models may be trained by computing such derivatives applied to samples from the model.
Training the model induces a representation h of the data v. We can often use Ehp(h|v)[h] as a set of features to describe v.
Overall, the RBM demonstrates the typical deep learning approach to graphical models: representation learning accomplished via layers of latent variables, combined with efficient interactions between layers parametrized by matrices.
The language of graphical models provides an elegant, flexible and clear language for describing probabilistic models. In the chapters ahead, we use this language, among other perspectives, to describe a wide variety of deep probabilistic models.

588

