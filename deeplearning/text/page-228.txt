CHAPTER 6. DEEP FEEDFORWARD NETWORKS

Algorithm 6.4 Backward computation for the deep neural network of algorithm 6.3, which uses in addition to the input x a target y. This computation yields the gradients on the activations a(k) for each layer k, starting from the output layer and going backwards to the first hidden layer. From these gradients, which can be interpreted as an indication of how each layer's output should change to reduce error, one can obtain the gradient on the parameters of each layer. The gradients on weights and biases can be immediately used as part of a stochastic gradient update (performing the update right after the gradients have been computed) or used with other gradient-based optimization methods.

After the forward computation, compute the gradient on the output layer:

g  y^J = y^L(y^, y) for k = l, l - 1, . . . , 1 do

Convert the gradient on the layer's output into a gradient into the pre-

nonlinearity activation (element-wise multiplication if f is element-wise): g  a(k) J = g  f (a(k)) Compute gradients on weights and biases (including the regularization term,

where needed):

 J (k) = g +  (k)()

b

b


W

J (k)

=

g

h(k-1)

+


W

(k)

()

Propagate the gradients w.r.t. the next lower-level hidden layer's activations:

g  h(k-1) J = W(k) g end for

213

